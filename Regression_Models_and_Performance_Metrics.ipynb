{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Regression Models and Performance Metrics |\n"
      ],
      "metadata": {
        "id": "8860i8Odvw5V"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2e57092"
      },
      "source": [
        "**Question 1:** What is Simple Linear Regression (SLR)? Explain its purpose."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adbf3252"
      },
      "source": [
        "**Answer 1:**\n",
        "Simple Linear Regression (SLR) is a method used to understand how two things are related — for example, how studying time affects exam scores.\n",
        "\n",
        "In simple words, it does three main things:\n",
        "\n",
        "Finds a relationship:\n",
        "It draws the best possible straight line through your data points to show how one variable changes with the other.\n",
        "\n",
        "Makes predictions:\n",
        "Once you have that line, you can use it to guess future values — like predicting a student’s score if you know how many hours they studied.\n",
        "\n",
        "Shows strength and direction:\n",
        "It helps you see if the relationship is strong or weak, and whether one value goes up when the other goes up (positive relationship) or down (negative relationship)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3d02dbe0"
      },
      "source": [
        "**Question 2:** What are the key assumptions of Simple Linear Regression?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "140e3e6f"
      },
      "source": [
        "**Answer 2:**\n",
        "Simple Linear Regression (SLR) works properly only when certain conditions — called assumptions — are met. If these assumptions are broken, the results might be misleading.\n",
        "\n",
        "Here are the main ones explained simply:\n",
        "\n",
        "Linearity:\n",
        "The relationship between the two variables should form a straight line — not a curve. In other words, as one variable increases or decreases, the other should change in a consistent, linear way.\n",
        "\n",
        "Independence of Errors:\n",
        "The errors (the differences between the actual and predicted values) shouldn’t affect each other. Each observation should be independent — one data point’s error shouldn’t depend on another’s.\n",
        "\n",
        "Homoscedasticity (Equal Spread of Errors):\n",
        "The errors should have a consistent spread across all values of the independent variable. Basically, the “scatter” of points around the line should look even — not tighter in some places and wider in others.\n",
        "\n",
        "Normality of Errors:\n",
        "The errors should follow a normal (bell-shaped) distribution. This helps ensure the regression results are accurate and reliable.\n",
        "\n",
        "No Multicollinearity:\n",
        "This mostly matters when you have more than one predictor variable (in multiple regression). In SLR, since there’s only one predictor, this isn’t a concern — but it’s good to keep in mind if you move to more complex models later."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30de8230"
      },
      "source": [
        "**Question 3:** Write the mathematical equation for a simple linear regression model and explain each term."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2babde1"
      },
      "source": [
        "**Answer 3:**\n",
        "\n",
        "The mathematical equation for a simple linear regression model is typically written as:\n",
        "\n",
        "$y = \\beta_0 + \\beta_1x + \\epsilon$\n",
        "\n",
        "Let's break down each term:\n",
        "\n",
        "*   **$y$**: This is the **dependent variable** (also called the response variable or outcome variable). It's the variable you are trying to predict or explain.\n",
        "*   **$x$**: This is the **independent variable** (also called the predictor variable or explanatory variable). This is the variable you are using to predict or explain $y$.\n",
        "*   **$\\beta_0$ (Beta-zero)**: This is the **y-intercept**. It represents the expected value of $y$ when $x$ is equal to 0. In some contexts, it might not have a meaningful interpretation if $x=0$ is outside the range of your data.\n",
        "*   **$\\beta_1$ (Beta-one)**: This is the **slope** of the regression line. It represents the change in the expected value of $y$ for a one-unit increase in $x$. It tells you the direction and strength of the linear relationship between $x$ and $y$.\n",
        "*   **$\\epsilon$ (epsilon)**: This is the **error term** (also called the residual). It represents the part of $y$ that cannot be explained by the linear relationship with $x$. It includes all other factors that influence $y$ but are not included in the model, as well as random variability.\n",
        "\n",
        "Essentially, the equation says that the dependent variable ($y$) is a linear function of the independent variable ($x$), plus some random error ($\\epsilon$). The line is defined by the intercept ($\\beta_0$) and the slope ($\\beta_1$)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81751f69"
      },
      "source": [
        "**Question 4:** Provide a real-world example where simple linear regression can be applied."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "beb525a1"
      },
      "source": [
        "**Answer 4:**\n",
        "A simple and relatable example of Simple Linear Regression (SLR) is the relationship between hours studied and exam scores.\n",
        "\n",
        "Dependent Variable (y): Exam Score\n",
        "\n",
        "Independent Variable (x): Hours Studied\n",
        "\n",
        "Imagine you collect data from several students on how long they studied and what scores they got. Using SLR, you can:\n",
        "\n",
        "Find the trend:\n",
        "Check if studying more hours generally leads to higher exam scores.\n",
        "\n",
        "Measure the effect:\n",
        "Use the regression line to see how much a student’s score is likely to increase for each extra hour of study — this is what the slope (β₁) represents.\n",
        "\n",
        "Make predictions:\n",
        "Once the model is ready, you can predict a student’s expected score if you know how many hours they studied.\n",
        "\n",
        "Other real-life examples of Simple Linear Regression:\n",
        "\n",
        "Advertising budget vs. sales revenue\n",
        "\n",
        "Years of work experience vs. salary\n",
        "\n",
        "Temperature vs. ice cream sales"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c342a4fd"
      },
      "source": [
        "**Question 5:** What is the method of least squares in linear regression?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1b31023a"
      },
      "source": [
        "**Answer 5:**\n",
        "The method of least squares is the most common way to find the best-fitting line in Simple Linear Regression.\n",
        "\n",
        "Imagine you’ve plotted your data points on a graph and drawn a line through them. Some points sit above the line, and some fall below it. The vertical distance between each point and the line is called a residual — it shows how far off the prediction is from the actual value.\n",
        "\n",
        "The method of least squares works by finding the line that makes these residuals as small as possible — specifically, it minimizes the sum of the squared residuals (the squares of those distances).\n",
        "\n",
        "Here’s why we square the residuals:\n",
        "\n",
        "To remove negative signs:\n",
        "Squaring ensures that positive and negative differences don’t cancel each other out.\n",
        "\n",
        "To give more weight to larger errors:\n",
        "Bigger mistakes count more, which pushes the line closer to the points that are far away.\n",
        "\n",
        "By doing this, we find the line that is, on average, closest to all data points — the one that best represents the relationship between the two variables."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "168f2d00"
      },
      "source": [
        "**Question 6:** What is Logistic Regression? How does it differ from Linear Regression?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ab21fcd2"
      },
      "source": [
        "**Answer 6:**\n",
        "\n",
        "**Logistic Regression** is a statistical method used for **binary classification**. This means it's used when the outcome variable you're trying to predict has only two possible categories (e.g., yes/no, true/false, spam/not spam, win/lose). Instead of predicting a continuous value like Linear Regression, Logistic Regression predicts the **probability** that a given input belongs to a particular category.\n",
        "\n",
        "**How it differs from Linear Regression:**\n",
        "\n",
        "The key difference lies in the type of dependent variable they handle and the output they produce:\n",
        "\n",
        "*   **Linear Regression:** Used for predicting a **continuous dependent variable**. The output is a continuous value. The equation is linear: $y = \\beta_0 + \\beta_1x + \\epsilon$.\n",
        "*   **Logistic Regression:** Used for predicting a **categorical dependent variable (specifically binary)**. The output is a probability (a value between 0 and 1) that the input belongs to the positive class. It uses a **sigmoid function** (also called the logistic function) to map the output of a linear equation to a probability. The basic form involves transforming the linear combination of inputs: $P(y=1|x) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1x)}}$.\n",
        "\n",
        "In simpler terms:\n",
        "\n",
        "*   **Linear Regression** is like drawing a straight line to predict a number.\n",
        "*   **Logistic Regression** is like using an \"S\"-shaped curve (the sigmoid function) to predict the likelihood of something belonging to one of two groups.\n",
        "\n",
        "Think of it this way: If you want to predict a person's height based on their age, you'd use Linear Regression (height is continuous). If you want to predict if a person will buy a product based on their browsing history, you'd use Logistic Regression (buy or not buy is binary)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0f0b831e"
      },
      "source": [
        "**Question 7:** Name and briefly describe three common evaluation metrics for regression models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cac529aa"
      },
      "source": [
        "**Answer 7:**\n",
        "\n",
        "When we build a regression model, we need to figure out how well it's performing. Evaluation metrics help us measure the accuracy of our predictions. Here are three common ones:\n",
        "\n",
        "1.  **Mean Absolute Error (MAE):**\n",
        "    *   **What it is:** MAE measures the average magnitude of the errors in a set of predictions, without considering their direction. It's the average of the absolute differences between the actual values and the predicted values.\n",
        "    *   **In simpler terms:** It tells you, on average, how far off your predictions are from the actual values, regardless of whether the prediction was too high or too low.\n",
        "    *   **Why use it:** It's easy to understand and interpret because it's in the same units as the dependent variable. It's also less sensitive to outliers than Mean Squared Error.\n",
        "\n",
        "2.  **Mean Squared Error (MSE):**\n",
        "    *   **What it is:** MSE measures the average of the squared errors. It's calculated by taking the average of the squared differences between the actual values and the predicted values.\n",
        "    *   **In simpler terms:** It's similar to MAE, but by squaring the errors, it gives more weight to larger errors.\n",
        "    *   **Why use it:** It's a widely used metric and is mathematically convenient for optimization. However, because the errors are squared, the units of MSE are not the same as the dependent variable, which can make it harder to interpret directly.\n",
        "\n",
        "3.  **Root Mean Squared Error (RMSE):**\n",
        "    *   **What it is:** RMSE is the square root of the Mean Squared Error.\n",
        "    *   **In simpler terms:** It brings the error back into the same units as the dependent variable, making it more interpretable than MSE. It still gives more weight to larger errors due to the squaring within the MSE calculation.\n",
        "    *   **Why use it:** It's a very common metric and is often preferred over MSE because it's in the same units as the original data. It represents the standard deviation of the residuals.\n",
        "\n",
        "Choosing the right metric depends on the specific problem and what aspects of the model's performance you want to emphasize."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33d2ac63"
      },
      "source": [
        "**Question 8:** What is the purpose of the R-squared metric in regression analysis?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5e69af0"
      },
      "source": [
        "**Answer 8:**\n",
        "\n",
        "**R-squared (R²)**, also known as the **coefficient of determination**, is a key metric in regression analysis that tells us how well the independent variable(s) in our model explain the variation in the dependent variable.\n",
        "\n",
        "**Its main purpose is to:**\n",
        "\n",
        "*   **Measure the proportion of variance explained:** R-squared represents the proportion of the variance in the dependent variable that is predictable from the independent variable(s). In simpler terms, it tells you how much of the \"jiggle\" or spread in your outcome variable is accounted for by the factors you included in your model.\n",
        "*   **Assess the goodness of fit:** A higher R-squared value generally indicates a better fit of the model to the data. An R-squared of 1 means the model perfectly explains all the variation in the dependent variable, while an R-squared of 0 means the model explains none of the variation.\n",
        "*   **Provide a scale for model comparison (with caution):** While you can use R-squared to compare different models on the same dataset, it's important to be cautious. Adding more independent variables to a model will always increase R-squared, even if those variables are not truly related to the dependent variable. This is why adjusted R-squared is often preferred when comparing models with different numbers of predictors.\n",
        "\n",
        "**In simpler terms:** R-squared is like a score (from 0 to 1) that tells you how much of the variation in the thing you're trying to predict can be explained by the things you're using to predict it. A higher score means your model is doing a better job of capturing the patterns in the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8459a19a"
      },
      "source": [
        "**Question 9:** Write Python code to fit a simple linear regression model using scikit-learn and print the slope and intercept."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "db5bf8ad",
        "outputId": "b39dd589-340c-4222-e027-11fd67c11088"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Sample data (replace with your actual data)\n",
        "# Let's use the hours studied vs. exam score example\n",
        "hours_studied = np.array([2, 3, 4, 5, 6, 7, 8, 9, 10]).reshape(-1, 1) # Independent variable (x)\n",
        "exam_scores = np.array([50, 60, 65, 70, 75, 80, 85, 90, 95])       # Dependent variable (y)\n",
        "\n",
        "# Create a Linear Regression model\n",
        "model = LinearRegression()\n",
        "\n",
        "# Fit the model to the data\n",
        "model.fit(hours_studied, exam_scores)\n",
        "\n",
        "# Print the slope (coefficient) and intercept\n",
        "print(f\"Slope (coefficient): {model.coef_[0]:.2f}\")\n",
        "print(f\"Intercept: {model.intercept_:.2f}\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Slope (coefficient): 5.33\n",
            "Intercept: 42.44\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1a9d3354"
      },
      "source": [
        "**Question 10:** How do you interpret the coefficients in a simple linear regression model?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7d984f17"
      },
      "source": [
        "**Answer 10:**\n",
        "\n",
        "In a simple linear regression model, represented by the equation $y = \\beta_0 + \\beta_1x + \\epsilon$, the coefficients $\\beta_0$ and $\\beta_1$ have specific interpretations:\n",
        "\n",
        "1.  **Intercept ($\\beta_0$):**\n",
        "    *   **Interpretation:** The intercept represents the expected value of the dependent variable ($y$) when the independent variable ($x$) is equal to 0.\n",
        "    *   **Important Note:** The interpretation of the intercept is only meaningful if $x=0$ is a plausible or relevant value within the context of your data. In some cases, $x=0$ might be outside the range of your data or have no practical meaning (e.g., predicting height based on age, where age 0 might not be relevant). In such cases, the intercept might not have a direct, interpretable meaning on its own, but it is still necessary for defining the regression line.\n",
        "\n",
        "2.  **Slope ($\\beta_1$):**\n",
        "    *   **Interpretation:** The slope represents the **change** in the expected value of the dependent variable ($y$) for a **one-unit increase** in the independent variable ($x$).\n",
        "    *   **Direction and Magnitude:**\n",
        "        *   If $\\beta_1$ is positive, it indicates a positive linear relationship: as $x$ increases, $y$ is expected to increase.\n",
        "        *   If $\\beta_1$ is negative, it indicates a negative linear relationship: as $x$ increases, $y$ is expected to decrease.\n",
        "        *   The magnitude (absolute value) of $\\beta_1$ tells you how much $y$ is expected to change for each unit change in $x$. A larger absolute value indicates a steeper slope and a stronger linear relationship.\n",
        "\n",
        "**Example (using the hours studied vs. exam score example):**\n",
        "\n",
        "If your fitted model is: `Exam Score = 42.44 + 5.33 * Hours Studied`\n",
        "\n",
        "*   **Intercept (42.44):** This would suggest that a student who studies 0 hours is expected to get an exam score of approximately 42.44. However, studying 0 hours might not be realistic, so this interpretation should be taken with caution.\n",
        "*   **Slope (5.33):** This means that for every additional hour a student studies, their exam score is expected to increase by approximately 5.33 points.\n",
        "\n",
        "Understanding these interpretations is crucial for drawing meaningful conclusions from your simple linear regression model."
      ]
    }
  ]
}